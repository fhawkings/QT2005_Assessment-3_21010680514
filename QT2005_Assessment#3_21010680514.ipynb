{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2475ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim import corpora, models\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "import networkx as nx\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.metrics import jaccard_distance\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "nltk.download('vader_lexicon')\n",
    "punct = list(string.punctuation)\n",
    "!pip install PRAW\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import praw\n",
    "import datetime\n",
    "import nltk.sentiment.vader as vd\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import IFrame\n",
    "import random\n",
    "from collections import Counter\n",
    "import random\n",
    "from random import sample\n",
    "from scipy.stats import entropy\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader_lexicon = SentimentIntensityAnalyzer()\n",
    "from wordcloud import WordCloud\n",
    "!pip install wordcloud\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objs as go\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import plotly.express as px\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from sklearn.cluster import KMeans\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36354300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing the bandwidth of the jupiter notebook to allow for the qaunitities of data\n",
    "%config NotebookApp.iopub_data_rate_limit = 10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb1e46",
   "metadata": {},
   "source": [
    "# FICTION TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing the search parameters for the Gutendex API\n",
    "params_g = {'topic':'fiction'}\n",
    "\n",
    "# Define the base URL for the Gutendex API\n",
    "gut = 'https://gutendex.com/books/'\n",
    "\n",
    "# Send a GET request to the Gutendex API with the search parameters\n",
    "rg = requests.get(url = gut, params = params_g)\n",
    "\n",
    "# Get the response from the API in JSON format\n",
    "rg = rg.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the desktop directory where the text files will be saved\n",
    "desktop_path = '/filepathname/'\n",
    "\n",
    "# Define a list of URLs for the text files to be downloaded\n",
    "urls = [\n",
    "    'https://www.gutenberg.org/ebooks/84.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/36.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/42324.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/41445.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/21279.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/164.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/5230.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/159.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/32032.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/61309.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/86.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/69762.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/139.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/201.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/18857.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/69703.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/19141.txt.utf-8',\n",
    "    'https://www.gutenberg.org/ebooks/61963.txt.utf-8',\n",
    "    \n",
    "]\n",
    "\n",
    "# Loop through each URL in the list of URLs\n",
    "for url in urls:\n",
    "    # Send a GET request to the URL to download the text file\n",
    "    response = requests.get(url)\n",
    "    # Get the filename from the URL and use it to create the name of the local file\n",
    "    filename = f\"{url.split('/')[-1].split('.')[0]}.txt\"\n",
    "    # Combine the desktop path with the filename to create the full path to the local file\n",
    "    filepath = os.path.join(desktop_path, filename)\n",
    "    # Write the contents of the downloaded text file to the local file\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to hold the data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through each text file and read it into a pandas dataframe\n",
    "for file in glob.glob(\"//filepathname/*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        # create a new dataframe with the file contents and add it to the main dataframe\n",
    "        df_new = pd.DataFrame({'text': [content]})\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying ascii to entire dataframe\n",
    "df = df.applymap(lambda x: x.encode('ascii', 'ignore').decode('ascii') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with a single row that contains all the text from the original DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text_combined': [df['text'].sum()]  # Sum all the strings in the 'text' column and put the result in a list\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf60583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into individual words\n",
    "df['words'] = df['text_combined'].str.split()\n",
    "\n",
    "# Create a new DataFrame with one row for each word\n",
    "new_df = df.explode('words')\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "new_df = new_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert NLTK part-of-speech tags to WordNet part-of-speech tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # By default, assume the word is a noun\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize and lemmatize the 'text' column, and store the result in a new column called 'tokenized_words'\n",
    "new_df['tokenized_words'] = new_df['words'].str.lower().apply(nltk.word_tokenize).apply(nltk.pos_tag).apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80968e12",
   "metadata": {},
   "source": [
    "# FICTION ANALYSIS VAD, TI-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b726858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAD scores from the Warriner rescale\n",
    "vad = pd.read_csv('//filepathname//Warriner_rescale.csv', index_col=0)\n",
    "vad = vad[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vad.columns = ['valence', 'arousal', 'dominance']\n",
    "\n",
    "# Define a function to get the VAD scores for a given word\n",
    "def get_vad_scores(word):\n",
    "    if word.lower() in vad.index:\n",
    "        scores = vad.loc[word.lower()].values\n",
    "        return scores\n",
    "    else:\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    \n",
    "# Create a new column in the dataframe to store the VAD scores\n",
    "new_df['vad_scores'] = [[] for _ in range(len(new_df))]\n",
    "\n",
    "# Iterate over each row in the dataframe and calculate VAD scores for each tokenized word\n",
    "for i, row in new_df.iterrows():\n",
    "    vad_scores = []\n",
    "    for word in row['tokenized_words']:\n",
    "        vad_scores.append(get_vad_scores(word))\n",
    "    new_df.at[i, 'vad_scores'] = vad_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named get_vad_scores that takes a word as input\n",
    "def get_vad_scores(word):\n",
    "    # Try to get the VAD scores for the word from the vad DataFrame\n",
    "    try:\n",
    "        # Lookup the word in the vad DataFrame and retrieve the VAD scores as a NumPy array\n",
    "        # The .lower() method is used to convert the word to lowercase for case-insensitive matching\n",
    "        scores = vad.loc[word.lower()].values\n",
    "        # Return the VAD scores as a NumPy array\n",
    "        return scores\n",
    "    # If the word is not in the vad DataFrame, catch the KeyError exception\n",
    "    except KeyError:\n",
    "        # Return None if the word is not in the vad DataFrame\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3988b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty lists to store the VAD scores for each word in the tokenized_words column of new_df\n",
    "\n",
    "valence_scores = []\n",
    "arousal_scores = []\n",
    "dominance_scores = []\n",
    "\n",
    "#Iterate over each row in new_df\n",
    "\n",
    "for i, row in new_df.iterrows():\n",
    "# Iterate over each word in the tokenized_words column of the current row\n",
    "    for word in row['tokenized_words']:\n",
    "    # Call the get_vad_scores function to get the VAD scores for the current word\n",
    "        score = get_vad_scores(word)\n",
    "        # If the get_vad_scores function returns a valid VAD score, append the scores to the appropriate list\n",
    "        if score is not None:\n",
    "            valence_scores.append(score[0])\n",
    "            arousal_scores.append(score[1])\n",
    "            dominance_scores.append(score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d059d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the valence_scores list\n",
    "mean_valence = np.mean(valence_scores)\n",
    "\n",
    "# Calculate the median of the valence_scores list\n",
    "median_valence = np.median(valence_scores)\n",
    "\n",
    "# Calculate the standard deviation of the valence_scores list\n",
    "std_valence = np.std(valence_scores)\n",
    "\n",
    "# Print the calculated measures\n",
    "print(\"Mean Valence Score:\", mean_valence)\n",
    "print(\"Median Valence Score:\", median_valence)\n",
    "print(\"Standard Deviation of Valence Scores:\", std_valence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on valence scores\n",
    "X = np.array(valence_scores).reshape(-1, 1)\n",
    "y = np.array(dominance_scores)  # replace dominance_scores with your target variable\n",
    "model = LinearRegression().fit(X, y)\n",
    "r_sq = model.score(X, y)\n",
    "\n",
    "print(\"R-squared value:\", r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d984e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on arousal scores\n",
    "X = np.array(arousal_scores).reshape(-1, 1)\n",
    "y = np.array(dominance_scores)  # replace dominance_scores with your target variable\n",
    "model = LinearRegression().fit(X, y)\n",
    "r_sq = model.score(X, y)\n",
    "\n",
    "print(\"R-squared value:\", r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e160b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on arousal scores\n",
    "X = np.array(valence_scores).reshape(-1, 1)\n",
    "y = np.array(arousal_scores)  # replace dominance_scores with your target variable\n",
    "model = LinearRegression().fit(X, y)\n",
    "r_sq = model.score(X, y)\n",
    "\n",
    "print(\"R-squared value:\", r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e5524",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a scatter plot of valence and arousal scores\n",
    "plt.scatter(valence_scores, arousal_scores, color='blue', alpha=0.5)\n",
    "\n",
    "# set the axis labels and title\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Arousal')\n",
    "plt.title('Fiction VAD Scores')\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3223d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of valence and dominance scores\n",
    "plt.scatter(valence_scores, dominance_scores, color='blue', alpha=0.5)\n",
    "\n",
    "# set the axis labels and title\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Dominance')\n",
    "plt.title('Fiction VAD Scores')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91306d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of arousal and dominance scores\n",
    "plt.scatter(arousal_scores, dominance_scores, color='blue', alpha=0.5)\n",
    "\n",
    "# set the axis labels and title\n",
    "plt.xlabel('Arousal')\n",
    "plt.ylabel('Dominance')\n",
    "plt.title('Fiction VAD Scores')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a54c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for the 3D plot\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a new subplot with 3D projection\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the VAD scores for each word as a scatter plot\n",
    "ax.scatter(valence_scores, arousal_scores, dominance_scores)\n",
    "\n",
    "# Set the labels for the x, y, and z axes\n",
    "ax.set_xlabel('Valence')\n",
    "ax.set_ylabel('Arousal')\n",
    "ax.set_zlabel('Dominance')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd78303",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a trace for the scatter plot\n",
    "trace = go.Scatter3d(\n",
    "    x=valence_scores,\n",
    "    y=arousal_scores,\n",
    "    z=dominance_scores,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=valence_scores,  # Set color to valence scores\n",
    "        colorscale='Viridis',  # Set the color scale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the layout for the plot\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Valence'),\n",
    "        yaxis=dict(title='Arousal'),\n",
    "        zaxis=dict(title='Dominance')\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "# Create the figure and add the trace and layout\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of 10 documents\n",
    "documents = new_df['text_combined'].sample(n=10).tolist()\n",
    "\n",
    "# create a list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenize each document and remove stop words\n",
    "texts = [[word for word in word_tokenize(document.lower()) if word not in stop_words] for document in documents]\n",
    "\n",
    "# create a dictionary from the tokenized texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# create a corpus from the dictionary and tokenized texts\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# build the LDA model with 5 topics\n",
    "num_topics = 5\n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, random_state=42)\n",
    "\n",
    "# print the top 10 words for each topic\n",
    "for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    print(f'Topic {i}: {\" \".join([word[0] for word in topic])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdeced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the matrix\n",
    "doc_topic_matrix = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "    topic_probs_dict = dict(topic_probs)\n",
    "    row = [topic_probs_dict.get(j, 0) for j in range(num_topics)]\n",
    "    doc_topic_matrix.append(row)\n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(doc_topic_matrix, cmap='YlGnBu')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Document')\n",
    "plt.title(f'Topic Probabilities for {len(documents)} Documents')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the file path\n",
    "file_path = \"/Users/felixhawkings/Desktop/csv/new_df.csv\"\n",
    "\n",
    "# export the dataframe to CSV\n",
    "new_df.to_csv(file_path, index=False)\n",
    "\n",
    "# check if the file was saved successfully\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File saved successfully at {file_path}\")\n",
    "else:\n",
    "    print(f\"Error: file not saved at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923add5",
   "metadata": {},
   "source": [
    "# SCIENCE FICTION TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing the search parameters for the Gutendex API\n",
    "params_g = {'topic':'science fiction'}\n",
    "\n",
    "# Define the base URL for the Gutendex API\n",
    "gut = 'https://gutendex.com/books/'\n",
    "\n",
    "# Send a GET request to the Gutendex API with the search parameters\n",
    "rg = requests.get(url = gut, params = params_g)\n",
    "\n",
    "# Get the response from the API in JSON format\n",
    "rg = rg.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab008ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the desktop directory where the text files will be saved\n",
    "desktop_path = '/filepathname/'\n",
    "\n",
    "# Define a list of URLs for the text files to be downloaded\n",
    "urls = ['https://www.gutenberg.org/ebooks/2641.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/145.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/37106.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/16389.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/67979.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/6761.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/394.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/1259.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/84.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/1342.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/11.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/174.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/64317.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/46.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/345.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/25344.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/20228.txt.utf-8',\n",
    " 'https://www.gutenberg.org/ebooks/28054.txt.utf-8'\n",
    "]\n",
    "\n",
    "# Loop through each URL in the list of URLs\n",
    "for url in urls:\n",
    "    # Send a GET request to the URL to download the text file\n",
    "    response = requests.get(url)\n",
    "    # Get the filename from the URL and use it to create the name of the local file\n",
    "    filename = f\"{url.split('/')[-1].split('.')[0]}.txt\"\n",
    "    # Combine the desktop path with the filename to create the full path to the local file\n",
    "    filepath = os.path.join(desktop_path, filename)\n",
    "    # Write the contents of the downloaded text file to the local file\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacf739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to hold the data\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "# loop through each text file and read it into a pandas dataframe\n",
    "for file in glob.glob(\"//filepathname//*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        # create a new dataframe with the file contents and add it to the main dataframe\n",
    "        df2_new = pd.DataFrame({'text': [content]})\n",
    "        df2 = pd.concat([df2, df2_new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82064e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying ascii to entire dataframe\n",
    "df2 = df2.applymap(lambda x: x.encode('ascii', 'ignore').decode('ascii') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns={'text': 'text2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407022b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with a single row that contains all the text from the original DataFrame\n",
    "df2 = pd.DataFrame({\n",
    "    'text2_combined': [df2['text2'].sum()]  # Sum all the strings in the 'text' column and put the result in a list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89edb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into individual words\n",
    "df2['words2'] = df2['text2_combined'].str.split()\n",
    "\n",
    "# Create a new DataFrame with one row for each word\n",
    "new_df2 = df2.explode('words2')\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "new_df2 = new_df2.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0266b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert NLTK part-of-speech tags to WordNet part-of-speech tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # By default, assume the word is a noun\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize and lemmatize the 'text' column, and store the result in a new column called 'tokenized_words'\n",
    "new_df2['tokenized2_words'] = new_df2['words2'].str.lower().apply(nltk.word_tokenize).apply(nltk.pos_tag).apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6b094",
   "metadata": {},
   "source": [
    "# SCIENCE FICTION ANALYSIS VAD, TI-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAD scores from the Warriner rescale\n",
    "vad = pd.read_csv('//filepathname//Warriner_rescale.csv', index_col=0)\n",
    "vad = vad[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vad.columns = ['valence', 'arousal', 'dominance']\n",
    "\n",
    "# Define a function to get the VAD scores for a given word\n",
    "def get_vad_scores(word):\n",
    "    if word.lower() in vad.index:\n",
    "        scores = vad.loc[word.lower()].values\n",
    "        return scores\n",
    "    else:\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    \n",
    "# Create a new column in the dataframe to store the VAD scores\n",
    "new_df2['vad2_scores'] = [[] for _ in range(len(new_df2))]\n",
    "\n",
    "# Iterate over each row in the dataframe and calculate VAD scores for each tokenized word\n",
    "for i, row in new_df2.iterrows():\n",
    "    vad2_scores = []\n",
    "    for word in row['tokenized2_words']:\n",
    "        vad2_scores.append(get_vad_scores(word))\n",
    "    new_df2.at[i, 'vad2_scores'] = vad2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0164d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named get_vad_scores that takes a word as input\n",
    "def get_vad_scores(word):\n",
    "    # Try to get the VAD scores for the word from the vad DataFrame\n",
    "    try:\n",
    "        # Lookup the word in the vad DataFrame and retrieve the VAD scores as a NumPy array\n",
    "        # The .lower() method is used to convert the word to lowercase for case-insensitive matching\n",
    "        scores2 = vad.loc[word.lower()].values\n",
    "        # Return the VAD scores as a NumPy array\n",
    "        return scores2\n",
    "    # If the word is not in the vad DataFrame, catch the KeyError exception\n",
    "    except KeyError:\n",
    "        # Return None if the word is not in the vad DataFrame\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty lists to store the VAD scores for each word in the tokenized_words column of new_df\n",
    "\n",
    "valence_scores2 = []\n",
    "arousal_scores2 = []\n",
    "dominance_scores2 = []\n",
    "\n",
    "#Iterate over each row in new_df\n",
    "\n",
    "for i, row in new_df2.iterrows():\n",
    "# Iterate over each word in the tokenized_words column of the current row\n",
    "    for word in row['tokenized2_words']:\n",
    "    # Call the get_vad_scores function to get the VAD scores for the current word\n",
    "        score = get_vad_scores(word)\n",
    "        # If the get_vad_scores function returns a valid VAD score, append the scores to the appropriate list\n",
    "        if score is not None:\n",
    "            valence_scores2.append(score[0])\n",
    "            arousal_scores2.append(score[1])\n",
    "            dominance_scores2.append(score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the valence_scores list\n",
    "mean_valence2 = np.mean(valence_scores2)\n",
    "\n",
    "# Calculate the median of the valence_scores list\n",
    "median_valence2 = np.median(valence_scores2)\n",
    "\n",
    "# Calculate the standard deviation of the valence_scores list\n",
    "std_valence2 = np.std(valence_scores2)\n",
    "\n",
    "# Print the calculated measures\n",
    "print(\"Mean Valence Score:\", mean_valence2)\n",
    "print(\"Median Valence Score:\", median_valence2)\n",
    "print(\"Standard Deviation of Valence Scores:\", std_valence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on valence scores\n",
    "X = np.array(valence_scores2).reshape(-1, 1)\n",
    "y = np.array(dominance_scores2)  # replace dominance_scores with your target variable\n",
    "model = LinearRegression().fit(X, y)\n",
    "r_sq = model.score(X, y)\n",
    "\n",
    "print(\"R-squared value:\", r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on arousal scores\n",
    "X = np.array(arousal_scores2).reshape(-1, 1)\n",
    "y = np.array(dominance_scores2)  # replace dominance_scores with your target variable\n",
    "model = LinearRegression().fit(X, y)\n",
    "r_sq = model.score(X, y)\n",
    "\n",
    "print(\"R-squared value:\", r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on arousal scores\n",
    "X = np.array(valence_scores2).reshape(-1, 1)\n",
    "y = np.array(arousal_scores2)  # replace dominance_scores with your target variable\n",
    "model = LinearRegression().fit(X, y)\n",
    "r_sq = model.score(X, y)\n",
    "\n",
    "print(\"R-squared value:\", r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of valence and arousal scores\n",
    "plt.scatter(valence_scores2, arousal_scores2, color='blue', alpha=0.5)\n",
    "\n",
    "# set the axis labels and title\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Arousal')\n",
    "plt.title('Science Fiction VAD Scores')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of valence and dominance scores\n",
    "plt.scatter(valence_scores2, dominance_scores2, color='blue', alpha=0.5)\n",
    "\n",
    "# set the axis labels and title\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Dominance')\n",
    "plt.title('Science Fiction VAD Scores')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a15492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of arousal and dominance scores\n",
    "plt.scatter(arousal_scores2, dominance_scores2, color='blue', alpha=0.5)\n",
    "\n",
    "# set the axis labels and title\n",
    "plt.xlabel('Arousal')\n",
    "plt.ylabel('Dominance')\n",
    "plt.title('Science Fiction VAD Scores')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for the 3D plot\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a new subplot with 3D projection\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the VAD scores for each word as a scatter plot\n",
    "ax.scatter(valence_scores2, arousal_scores2, dominance_scores2)\n",
    "\n",
    "# Set the labels for the x, y, and z axes\n",
    "ax.set_xlabel('Valence')\n",
    "ax.set_ylabel('Arousal')\n",
    "ax.set_zlabel('Dominance')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trace for the scatter plot\n",
    "trace = go.Scatter3d(\n",
    "    x=valence_scores2,\n",
    "    y=arousal_scores2,\n",
    "    z=dominance_scores2,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=valence_scores2,  # Set color to valence scores\n",
    "        colorscale='reds',  # Set the color scale to red-pink\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the layout for the plot\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Valence'),\n",
    "        yaxis=dict(title='Arousal'),\n",
    "        zaxis=dict(title='Dominance')\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "# Create the figure and add the trace and layout\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adedfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of 10 documents\n",
    "documents2 = new_df2['text2_combined'].sample(n=10).tolist()\n",
    "\n",
    "# create a list of stop words\n",
    "stop_words2 = set(stopwords.words('english'))\n",
    "\n",
    "# tokenize each document and remove stop words\n",
    "texts2 = [[word for word in word_tokenize(document.lower()) if word not in stop_words2] for document in documents2]\n",
    "\n",
    "# create a dictionary from the tokenized texts\n",
    "dictionary2 = corpora.Dictionary(texts2)\n",
    "\n",
    "# create a corpus from the dictionary and tokenized texts\n",
    "corpus = [dictionary2.doc2bow(text) for text in texts2]\n",
    "\n",
    "# build the LDA model with 5 topics\n",
    "num_topics = 5\n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary2, num_topics=num_topics, passes=10, random_state=42)\n",
    "\n",
    "# print the top 10 words for each topic\n",
    "for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    print(f'Topic {i}: {\" \".join([word[0] for word in topic])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the matrix\n",
    "doc_topic_matrix = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "    topic_probs_dict = dict(topic_probs)\n",
    "    row = [topic_probs_dict.get(j, 0) for j in range(num_topics)]\n",
    "    doc_topic_matrix.append(row)\n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(doc_topic_matrix, cmap='YlGnBu')\n",
    "plt.xlabel('Topic2')\n",
    "plt.ylabel('Document2')\n",
    "plt.title(f'Science Fiction Topic Probabilities for {len(documents2)} Documents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('QT2005_Assessment#3_21010680514_Fiction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96917e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2.to_csv('QT2005_Assessment#3_21010680514_Science_Fiction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
